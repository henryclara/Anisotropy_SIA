{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bcdd14",
   "metadata": {},
   "source": [
    "### This code to calculate the shallow ice approximation (SIA) velocities for a specific ice rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cbdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.interpolate\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from matplotlib import ticker, cm\n",
    "from numpy.linalg import eig\n",
    "import math as maths\n",
    "from matplotlib import style\n",
    "import scipy.ndimage\n",
    "import scipy as sp\n",
    "style.use('default') or plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ef7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in bed, grounding line, ice surface and velocity data\n",
    "\n",
    "data_bed = pd.read_csv('../data/HIR_BedElevation_Bedmachine.csv', delimiter=' ')\n",
    "data_GL = pd.read_csv('../data/HIR_GL.csv', delim_whitespace=True)\n",
    "data_surf = pd.read_csv('../data/HIR_SurfaceElevation_REMA.csv', delimiter=' ')\n",
    "data_vel = pd.read_csv('../data/HIR_SurfaceVelocity.csv', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of data points for the surface for computational efficiency. \n",
    "# The plan is to run the code on the cluster using all data points\n",
    "\n",
    "data_surf = data_surf.iloc[::100, :]\n",
    "data_surf = data_surf.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64480a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the grounding line file to only include relevant data points\n",
    "\n",
    "data_GL_reduced = data_GL.loc[56:141]\n",
    "\n",
    "# Add an extra data point in the domain corner and create polygon\n",
    "\n",
    "new_row = pd.DataFrame({'X':data_GL['X'][56], 'Y':data_GL['Y'][141]}, index=[0])\n",
    "data_GL_reduced_extra_point = pd.concat([new_row,data_GL_reduced.loc[:]]).reset_index(drop=True)\n",
    "poly = Polygon(zip(list(data_GL_reduced_extra_point['X']), list(data_GL_reduced_extra_point['Y'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose domain coordinates, create regular grid and interpolate unstructured data onto regular grid\n",
    "\n",
    "xmin, xmax = 770000, 825000\n",
    "ymin, ymax = 1960000, 2020000\n",
    "dist = 100\n",
    "nx, ny = int((xmax - xmin)/dist + 1), int((ymax - ymin)/dist + 1)\n",
    "\n",
    "x = np.linspace(xmin, xmax, nx)\n",
    "y = np.linspace(ymin, ymax, ny)\n",
    "grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "# Grid surface and bed onto regular grid. Will try a higher order method once everything is working\n",
    "surf = scipy.interpolate.griddata((data_surf[\"X\"], data_surf[\"Y\"]), data_surf[\"surf\"], (grid_x, grid_y), method='linear')\n",
    "bed = scipy.interpolate.griddata((data_bed[\"X\"], data_bed[\"Y\"]), data_bed[\"Z\"], (grid_x, grid_y), method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aadce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the surface data\n",
    "\n",
    "sigma_x = 10.0\n",
    "sigma_y = 10.0\n",
    "\n",
    "sigma = [sigma_x, sigma_y]\n",
    "surf = sp.ndimage.gaussian_filter(surf, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ice thickness\n",
    "\n",
    "height = surf - bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc25f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the above array and set all value to zero\n",
    "slope_x = np.zeros_like(surf)\n",
    "slope_y = np.zeros_like(surf)\n",
    "\n",
    "# Code to calculate slope at each node (find slope between previous and next point)\n",
    "\n",
    "I = len(slope_x)\n",
    "J = len(slope_x[0])\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        z10 = surf[i-1][j]\n",
    "        z12 = surf[i+1][j]\n",
    "        z01 = surf[i][j-1]\n",
    "        z21 = surf[i][j+1]\n",
    "        slope_x[i][j] = (z21 - z01)/(dist*2)\n",
    "        slope_y[i][j] = (z12 - z10)/(dist*2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SIA for each point\n",
    "\n",
    "A, rho, g, n = 4.6e-25, 910.0, 9.81, 3\n",
    "\n",
    "vel_x = np.zeros_like(surf)\n",
    "vel_y = np.zeros_like(surf)\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        grad = (slope_x[i][j]**2 + slope_y[i][j]**2)**((n-1.0)/2.0)\n",
    "        vel_x[i][j] = -((rho*g)**3.0)*(A/2) * slope_x[i][j] * grad * (height[i][j]**4)\n",
    "        vel_y[i][j] = -((rho*g)**3.0)*(A/2) * slope_y[i][j] * grad * (height[i][j]**4)\n",
    "\n",
    "vel_x = vel_x * (365.25*24*60*60)\n",
    "vel_y = vel_y * (365.25*24*60*60)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6004f76",
   "metadata": {},
   "source": [
    "# Remove data points outside of polygon\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        boolean = poly.contains(Point(x[i], y[j]))\n",
    "        if boolean == False:\n",
    "            surf[j][i] = np.nan\n",
    "            vel_x[j][i] = np.nan\n",
    "            vel_y[j][i] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b34250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate horizontal strain rate tensor at each node\n",
    "\n",
    "def Strain(vx, vy, d):\n",
    "    vel_x = vx\n",
    "    vel_y = vx\n",
    "    exx = np.zeros_like(surf)\n",
    "    eyy = np.zeros_like(surf)\n",
    "    exy = np.zeros_like(surf)\n",
    "    for j in range(1, J-1):\n",
    "        for i in range(1, I-1):\n",
    "            di = d\n",
    "            dj = d\n",
    "            while i<=di or i>=I-di:\n",
    "                di = di - 1\n",
    "            while j<=dj or j>=J-dj:\n",
    "                dj = dj - 1\n",
    "            while any([ np.isnan(vel_x[i][j+dj]), np.isnan(vel_x[i][j-dj])]) and dj > 1:\n",
    "                dj = dj - 1\n",
    "            while any([ np.isnan(vel_x[i+di][j]), np.isnan(vel_x[i-di][j])]) and di > 1:\n",
    "                di = di - 1\n",
    "            exx[i][j] = (vel_x[i][j+dj] - vel_x[i][j-dj])/(dj*dist*2)\n",
    "            eyy[i][j] = (vel_y[i+di][j] - vel_y[i-di][j])/(di*dist*2)\n",
    "            exy[i][j] = 0.5*((vel_y[i][j+dj] - vel_y[i][j-dj])/(dj*dist*2)\\\n",
    "                           + (vel_x[i+di][j] - vel_x[i-di][j])/(di*dist*2))\n",
    "    return exx, eyy, exy\n",
    "\n",
    "exx_1, eyy_1, exy_1 = Strain(vel_x, vel_y, 1)\n",
    "exx_5, eyy_5, exy_5 = Strain(vel_x, vel_y, 5)\n",
    "exx_10, eyy_10, exy_10 = Strain(vel_x, vel_y, 10)\n",
    "exx_15, eyy_15, exy_15 = Strain(vel_x, vel_y, 15)\n",
    "exx_20, eyy_20, exy_20 = Strain(vel_x, vel_y, 20)\n",
    "exx_25, eyy_25, exy_25 = Strain(vel_x, vel_y, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate strain rate eigenvectors and eigenvalues\n",
    "\n",
    "def Eigs(exx, eyy, exy):\n",
    "    v11 = np.zeros_like(surf) # First component of the first vector\n",
    "    v12 = np.zeros_like(surf) # Second component of the first vector\n",
    "    v21 = np.zeros_like(surf) # First component of the second vector\n",
    "    v22 = np.zeros_like(surf) # Second component of the second vector\n",
    "    e1 = np.zeros_like(surf)\n",
    "    e2 = np.zeros_like(surf)\n",
    "    for i in range(1, I-1):\n",
    "        for j in range(1, J-1):\n",
    "            if ((maths.isnan(exx[i][j])) or (maths.isnan(eyy[i][j])) or (maths.isnan(exy[i][j]))):\n",
    "                e1[i][j], e2[i][j] = np.nan, np.nan\n",
    "                v11[i][j], v12[i][j] = np.nan, np.nan\n",
    "                v21[i][j], v22[i][j] = np.nan, np.nan\n",
    "            else:\n",
    "                arr = np.array([[exx[i][j], exy[i][j]], \n",
    "                                [exy[i][j], eyy[i][j]]])\n",
    "                w,v = eig(arr)\n",
    "                e1[i][j], e2[i][j] = w\n",
    "                v11[i][j], v12[i][j] = v[0]\n",
    "                v21[i][j], v22[i][j] = v[1]\n",
    "    return e1, e2, v11, v12, v21, v22\n",
    "\n",
    "e1_1, e2_1, v11_1, v12_1, v21_1, v22_1 = Eigs(exx_1, eyy_1, exy_1)\n",
    "e1_5, e2_5, v11_5, v12_5, v21_5, v22_5 = Eigs(exx_5, eyy_5, exy_5)\n",
    "e1_10, e2_10, v11_10, v12_10, v21_10, v22_10 = Eigs(exx_10, eyy_10, exy_10)\n",
    "e1_20, e2_20, v11_20, v12_20, v21_20, v22_20 = Eigs(exx_20, eyy_20, exy_20)\n",
    "e1_25, e2_25, v11_25, v12_25, v21_25, v22_25 = Eigs(exx_25, eyy_25, exy_25)\n",
    "\n",
    "#for i in range(1, I-1):\n",
    "#    for j in range(1, J-1):\n",
    "#        if ((maths.isnan(exx[i][j])) or (maths.isnan(eyy[i][j])) or (maths.isnan(exy[i][j]))):\n",
    "#            break\n",
    "#        arr = np.array([[exx[i][j], exy[i][j]], \n",
    "#                        [exy[i][j], eyy[i][j]]])\n",
    "#        w,v = eig(arr)\n",
    "#        e1[i][j], e2[i][j] = w\n",
    "#        v11[i][j], v12[i][j] = v[0]\n",
    "#        v21[i][j], v22[i][j] = v[1]\n",
    "\n",
    "# Weight eigenvectors based on eigenvalues\n",
    "\n",
    "#v11 = np.multiply(v11,e1)\n",
    "#v12 = np.multiply(v12,e1)\n",
    "#v21 = np.multiply(v21,e2)\n",
    "#v22 = np.multiply(v22,e2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dilation rate\n",
    "\n",
    "def DilRate(e1, e2):\n",
    "    dil_rate = np.zeros_like(surf)\n",
    "    for i in range(1, I-1):\n",
    "        for j in range(1, J-1):\n",
    "            dil_rate[i][j] = e1[i][j] * e2[i][j]\n",
    "    return dil_rate\n",
    "\n",
    "dil_rate_1 = DilRate(e1_1, e2_1)\n",
    "dil_rate_5 = DilRate(e1_5, e2_5)\n",
    "dil_rate_10 = DilRate(e1_10, e2_10)\n",
    "dil_rate_20 = DilRate(e1_20, e2_20)\n",
    "dil_rate_25 = DilRate(e1_25, e2_25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of the eigenvector corresponding with the largest eigenvalue\n",
    "\n",
    "def MinMaxEigs(e1, e2, v11, v12, v21, v22):\n",
    "    v1_min = np.zeros_like(surf)\n",
    "    v2_min = np.zeros_like(surf)\n",
    "    v1_max = np.zeros_like(surf)\n",
    "    v2_max = np.zeros_like(surf)\n",
    "\n",
    "    for i in range(1, I-1):\n",
    "        for j in range(1, J-1):\n",
    "            if e1[i][j] > e2[i][j]:\n",
    "                v1_min[i][j] = v21[i][j]\n",
    "                v2_min[i][j] = v22[i][j]\n",
    "                v1_max[i][j] = v11[i][j]\n",
    "                v2_max[i][j] = v12[i][j]\n",
    "            else:\n",
    "                v1_min[i][j] = v11[i][j]\n",
    "                v2_min[i][j] = v12[i][j]\n",
    "                v1_max[i][j] = v21[i][j]\n",
    "                v2_max[i][j] = v22[i][j]\n",
    "    return v1_min, v2_min, v1_max, v2_max\n",
    "\n",
    "v1_min_1, v2_min_1, v1_max_1, v2_max_1 = MinMaxEigs(e1_1, e2_1, v11_1, v12_1, v21_1, v22_1)\n",
    "v1_min_5, v2_min_5, v1_max_5, v2_max_5 = MinMaxEigs(e1_5, e2_5, v11_5, v12_5, v21_5, v22_5)\n",
    "v1_min_10, v2_min_10, v1_max_10, v2_max_10 = MinMaxEigs(e1_10, e2_10, v11_10, v12_10, v21_10, v22_10)\n",
    "v1_min_20, v2_min_20, v1_max_20, v2_max_20 = MinMaxEigs(e1_20, e2_20, v11_20, v12_20, v21_20, v22_20)\n",
    "v1_min_25, v2_min_25, v1_max_25, v2_max_25 = MinMaxEigs(e1_25, e2_25, v11_25, v12_25, v21_25, v22_25)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total strain at each node - equivalent to the first tensor invariant\n",
    "\n",
    "def StrainMag(exx, eyy):\n",
    "    e_total = np.zeros_like(surf)\n",
    "    for i in range(1, I-1):\n",
    "        for j in range(1, J-1):\n",
    "            e_total[i][j] = (exx[i][j] + eyy[i][j])\n",
    "    return e_total\n",
    "\n",
    "e_total_1 = StrainMag(exx_1, eyy_1)\n",
    "e_total_5 = StrainMag(exx_5, eyy_5)\n",
    "e_total_10 = StrainMag(exx_10, eyy_10)\n",
    "e_total_20 = StrainMag(exx_20, eyy_20)\n",
    "e_total_25 = StrainMag(exx_25, eyy_25)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf861f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the second strain rate tensor invariant\n",
    "\n",
    "def StrainSecInv(exx, eyy, exy):\n",
    "    e_2nd_inv = np.zeros_like(surf)\n",
    "    for i in range(1, I-1):\n",
    "        for j in range(1, J-1):\n",
    "            e_2nd_inv[i][j] = exx[i][j]*eyy[i][j] - exy[i][j]**2.0\n",
    "    return e_2nd_inv\n",
    "\n",
    "e_2nd_inv_1 = StrainSecInv(exx_1, eyy_1, exy_1)\n",
    "e_2nd_inv_5 = StrainSecInv(exx_5, eyy_5, exy_5)\n",
    "e_2nd_inv_10 = StrainSecInv(exx_10, eyy_10, exy_10)\n",
    "e_2nd_inv_20 = StrainSecInv(exx_20, eyy_20, exy_20)\n",
    "e_2nd_inv_25 = StrainSecInv(exx_25, eyy_25, exy_25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2094da",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_2nd_inv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array with velocity magnitudes\n",
    "\n",
    "vel_mag = np.zeros_like(surf)\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        vel_mag[i][j] = np.sqrt(vel_x[i][j]**2 + vel_y[i][j]**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points outside of polygon\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        boolean = poly.contains(Point(x[i], y[j]))\n",
    "        if boolean == False:\n",
    "            surf[j][i] = \"nan\"\n",
    "            vel_mag[j][i] = \"nan\"\n",
    "            vel_x[j][i] = \"nan\"\n",
    "            vel_y[j][i] = \"nan\"\n",
    "\n",
    "def NaNify(v11,v12,v21,v22,v1_max,v2_max,e_total, e_2nd_inv):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            boolean = poly.contains(Point(x[i], y[j]))\n",
    "            if boolean == False:\n",
    "                v11[j][i] = \"nan\"\n",
    "                v12[j][i] = \"nan\"\n",
    "                v21[j][i] = \"nan\"\n",
    "                v22[j][i] = \"nan\"\n",
    "                v1_max[j][i] = \"nan\"\n",
    "                v2_max[j][i] = \"nan\"\n",
    "                e_total[j][i] = \"nan\"\n",
    "                e_2nd_inv[j][i] = \"nan\"\n",
    "\n",
    "NaNify(v11_1,v12_1,v21_1,v22_1,v1_max_1,v2_max_1,e_total_1, e_2nd_inv_1)\n",
    "NaNify(v11_5,v12_5,v21_5,v22_5,v1_max_5,v2_max_5,e_total_5, e_2nd_inv_5)\n",
    "NaNify(v11_10,v12_10,v21_10,v22_10,v1_max_10,v2_max_10,e_total_10, e_2nd_inv_10)\n",
    "NaNify(v11_20,v12_20,v21_20,v22_20,v1_max_20,v2_max_20,e_total_20, e_2nd_inv_20)\n",
    "NaNify(v11_25,v12_25,v21_25,v22_25,v1_max_25,v2_max_25,e_total_25, e_2nd_inv_25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d80605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide velocities by their magnitude for normalised vectors\n",
    "\n",
    "vel_x_norm = np.divide(vel_x, vel_mag, out=np.zeros_like(vel_x), where=vel_mag!=0)\n",
    "vel_y_norm = np.divide(vel_y, vel_mag, out=np.zeros_like(vel_y), where=vel_mag!=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37060d00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "levels1 = np.linspace(0,10,500)\n",
    "CS = ax.contourf(grid_x/1000-770, grid_y/1000-1960, vel_mag, levels=levels1, cmap=plt.cm.GnBu, locator=ticker.LogLocator(), extend='max')\n",
    "\n",
    "skip1 = (slice(None, None, 20), slice(None, None, 20))\n",
    "ax.quiver(grid_x[skip1]/1000-770, grid_y[skip1]/1000-1960, vel_x_norm[skip1], vel_y_norm[skip1], scale=40)\n",
    "ax.plot(data_GL_reduced['X']/1000-770, data_GL_reduced['Y']/1000-1960, color='black')\n",
    "ax.set_xlabel(r'$x$ [km]', size=20)\n",
    "ax.set_ylabel(r'$y$ [km]', size=20)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "cbar = fig.colorbar(CS, ticks=[0, 2, 4, 6, 8, 10])\n",
    "cbar.set_label('Velocity magnitude [ma$^{-1}$]', size=20)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "ax.set_xlim([0, 55])\n",
    "ax.set_ylim([0, 60])\n",
    "ax.set_xlabel(r'$x$ [km]', size=20)\n",
    "ax.set_ylabel(r'$y$ [km]', size=20)\n",
    "fig.savefig('HammarryggenVelocity.jpg', format='jpg', dpi=700, bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b802ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "#levels2 = np.linspace(-0.001,0.001,500)\n",
    "levels2 = np.linspace(-0.0005,0.0005,500)\n",
    "CS = ax.contourf(grid_x/1000-770, grid_y/1000-1960, e_total_1, levels=levels2, cmap=plt.cm.RdYlBu, extend='both')\n",
    "#CS = ax.contourf(grid_x, grid_y, e_total_1, levels=levels2, cmap=plt.cm.GnBu, locator=ticker.LogLocator(), extend='max')\n",
    "ax.set_xlim([0, 55])\n",
    "ax.set_ylim([0, 60])\n",
    "skip2 = (slice(None, None, 20), slice(None, None, 20))\n",
    "#ax.quiver(grid_x[skip1]/1000-770, grid_y[skip1]/1000-1960, vel_x_norm[skip1], vel_y_norm[skip1], scale=40)\n",
    "ax.quiver(grid_x[skip2]/1000-770, grid_y[skip2]/1000-1960, v1_max_20[skip2], v2_max_20[skip2], scale=40, pivot='mid', headlength=0, headwidth=1)\n",
    "ax.plot(data_GL_reduced['X']/1000-770, data_GL_reduced['Y']/1000-1960, color='black')\n",
    "ax.set_xlabel(r'$x$ [km]', size=20)\n",
    "ax.set_ylabel(r'$y$ [km]', size=20)\n",
    "cbar = fig.colorbar(CS)\n",
    "cbar.set_label('Strain rate [a$^{-1}$]', size=20)\n",
    "#ax.tick_params(color='w', labelcolor='w')\n",
    "fig.savefig('HammarryggenStrainRate.jpg', format='jpg', dpi=700, bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b088ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot second strain rate tensor invariant\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "#levels2 = np.linspace(-0.001,0.001,500)\n",
    "levels2 = np.linspace(-0.00000001,0.00000001,500)\n",
    "CS = ax.contourf(grid_x/1000-770, grid_y/1000-1960, e_2nd_inv_1, levels=levels2, cmap=plt.cm.RdYlBu, extend='both')\n",
    "#CS = ax.contourf(grid_x, grid_y, e_2nd_inv_1, cmap=plt.cm.GnBu, locator=ticker.LogLocator(), extend='max')\n",
    "#CS = ax.contourf(grid_x, grid_y, e_2nd_inv_1, cmap=plt.cm.GnBu, extend='max')\n",
    "ax.set_xlim([0, 55])\n",
    "ax.set_ylim([0, 60])\n",
    "skip2 = (slice(None, None, 20), slice(None, None, 20))\n",
    "#ax.quiver(grid_x[skip1]/1000-770, grid_y[skip1]/1000-1960, vel_x_norm[skip1], vel_y_norm[skip1], scale=40)\n",
    "ax.quiver(grid_x[skip2]/1000-770, grid_y[skip2]/1000-1960, v1_max_20[skip2], v2_max_20[skip2], scale=40, pivot='mid', headlength=0, headwidth=1)\n",
    "ax.plot(data_GL_reduced['X']/1000-770, data_GL_reduced['Y']/1000-1960, color='black')\n",
    "ax.set_xlabel(r'$x$ [km]', size=20)\n",
    "ax.set_ylabel(r'$y$ [km]', size=20)\n",
    "cbar = fig.colorbar(CS)\n",
    "cbar.set_label('Strain rate [a$^{-1}$]', size=20)\n",
    "#ax.tick_params(color='w', labelcolor='w')\n",
    "fig.savefig('HammarryggenStrainRate2ndInvariant.jpg', format='jpg', dpi=700, bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a237466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dilation rate (i.e. the change in \"area\" of the strain rate tensor)  dil_rate_1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "#levels2 = np.linspace(-0.001,0.001,500)\n",
    "levels2 = np.linspace(-0.00000001,0.00000001,500)\n",
    "CS = ax.contourf(grid_x/1000-770, grid_y/1000-1960, dil_rate_1, levels=levels2, cmap=plt.cm.RdYlBu, extend='both')\n",
    "#CS = ax.contourf(grid_x, grid_y, e_2nd_inv_1, cmap=plt.cm.GnBu, locator=ticker.LogLocator(), extend='max')\n",
    "#CS = ax.contourf(grid_x, grid_y, e_2nd_inv_1, cmap=plt.cm.GnBu, extend='max')\n",
    "ax.set_xlim([0, 55])\n",
    "ax.set_ylim([0, 60])\n",
    "skip2 = (slice(None, None, 20), slice(None, None, 20))\n",
    "#ax.quiver(grid_x[skip1]/1000-770, grid_y[skip1]/1000-1960, vel_x_norm[skip1], vel_y_norm[skip1], scale=40)\n",
    "ax.quiver(grid_x[skip2]/1000-770, grid_y[skip2]/1000-1960, v1_max_20[skip2], v2_max_20[skip2], scale=40, pivot='mid', headlength=0, headwidth=1)\n",
    "ax.plot(data_GL_reduced['X']/1000-770, data_GL_reduced['Y']/1000-1960, color='black')\n",
    "ax.set_xlabel(r'$x$ [km]', size=20)\n",
    "ax.set_ylabel(r'$y$ [km]', size=20)\n",
    "cbar = fig.colorbar(CS)\n",
    "cbar.set_label('Strain rate [a$^{-1}$]', size=20)\n",
    "#ax.tick_params(color='w', labelcolor='w')\n",
    "fig.savefig('HammarryggenStrainRateDilationRate.jpg', format='jpg', dpi=700, bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0aabe5f",
   "metadata": {},
   "source": [
    "# Reduce resolution\n",
    "\n",
    "def Res(r):\n",
    "    skip = (slice(None, None, r), slice(None, None, r))\n",
    "    vx + str(r) = vel_x[skip]\n",
    "    vy + str(r) = vel_y[skip]\n",
    "    vmag + str(r) = vel_mag[skip]\n",
    "    xcoord + str(r) = grid_x[skip]\n",
    "    ycoord + str(r) = grid_y[skip]\n",
    "    vx = vel_x + str(r).flatten()\n",
    "    vy = vel_y + str(r).flatten()\n",
    "    vmag = vel_mag + str(r).flatten()\n",
    "    xcoord = grid_x + str(r).flatten()\n",
    "    ycoord = grid_y + str(r).flatten()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e63ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a resolution of 2.5km\n",
    "\n",
    "skip = (slice(None, None, 25), slice(None, None, 25))\n",
    "vx25 = vel_x[skip]\n",
    "vy25 = vel_y[skip]\n",
    "vmag25 = vel_mag[skip]\n",
    "xcoord25 = grid_x[skip]\n",
    "ycoord25 = grid_y[skip]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv of the velocity data\n",
    "\n",
    "# Flatten each array\n",
    "\n",
    "vx = vx25.flatten()\n",
    "vy = vy25.flatten()\n",
    "vmag = vmag25.flatten()\n",
    "xcoord = xcoord25.flatten()\n",
    "ycoord = ycoord25.flatten()\n",
    "\n",
    "HammarryggenVelData = np.array([xcoord, ycoord, vx, vy, vmag ])\n",
    "HammarryggenVelData = HammarryggenVelData.T\n",
    "np.savetxt(\"HammarryggenVelData25.csv\", HammarryggenVelData, delimiter=\",\",  header=\"xcoord,ycoord,vx,vy,vmag\", comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f780421",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vx), len(vy), len(vmag), len(xcoord), len(ycoord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12238fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31121940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120fb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460b13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e46374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fb895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
