{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bcdd14",
   "metadata": {},
   "source": [
    "### This code to calculate the shallow ice approximation (SIA) velocities for a specific ice rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cbdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.interpolate\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from matplotlib import ticker, cm\n",
    "from numpy.linalg import eig\n",
    "import math as maths\n",
    "from matplotlib import style\n",
    "import scipy.ndimage\n",
    "import scipy as sp\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036ef7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in bed, grounding line, ice surface and velocity data\n",
    "\n",
    "data_bed = pd.read_csv('../data/HIR_BedElevation_Bedmachine.csv', delimiter=' ')\n",
    "data_GL = pd.read_csv('../data/HIR_GL.csv', delim_whitespace=True)\n",
    "data_surf = pd.read_csv('../data/HIR_SurfaceElevation_REMA.csv', delimiter=' ')\n",
    "data_vel = pd.read_csv('../data/HIR_SurfaceVelocity.csv', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ba388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of data points for the surface for computational efficiency. \n",
    "# The plan is to run the code on the cluster using all data points\n",
    "\n",
    "data_surf = data_surf.iloc[::100, :]\n",
    "data_surf = data_surf.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64480a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the grounding line file to only include relevant data points\n",
    "\n",
    "data_GL_reduced = data_GL.loc[56:141]\n",
    "\n",
    "# Add an extra data point in the domain corner and create polygon\n",
    "\n",
    "new_row = pd.DataFrame({'X':data_GL['X'][56], 'Y':data_GL['Y'][141]}, index=[0])\n",
    "data_GL_reduced_extra_point = pd.concat([new_row,data_GL_reduced.loc[:]]).reset_index(drop=True)\n",
    "poly = Polygon(zip(list(data_GL_reduced_extra_point['X']), list(data_GL_reduced_extra_point['Y'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7cb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose domain coordinates, create regular grid and interpolate unstructured data onto regular grid\n",
    "\n",
    "xmin, xmax = 770000, 825000\n",
    "ymin, ymax = 1960000, 2020000\n",
    "dist = 100\n",
    "nx, ny = int((xmax - xmin)/dist + 1), int((ymax - ymin)/dist + 1)\n",
    "\n",
    "x = np.linspace(xmin, xmax, nx)\n",
    "y = np.linspace(ymin, ymax, ny)\n",
    "grid_x, grid_y = np.meshgrid(x, y)\n",
    "\n",
    "# Grid surface and bed onto regular grid. Will try a higher order method once everything is working\n",
    "surf = scipy.interpolate.griddata((data_surf[\"X\"], data_surf[\"Y\"]), data_surf[\"surf\"], (grid_x, grid_y), method='linear')\n",
    "bed = scipy.interpolate.griddata((data_bed[\"X\"], data_bed[\"Y\"]), data_bed[\"Z\"], (grid_x, grid_y), method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78aadce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the surface data\n",
    "\n",
    "sigma_x = 10.0\n",
    "sigma_y = 10.0\n",
    "\n",
    "sigma = [sigma_x, sigma_y]\n",
    "surf = sp.ndimage.gaussian_filter(surf, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1061779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ice thickness\n",
    "\n",
    "height = surf - bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc25f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the above array and set all value to zero\n",
    "slope_x = np.zeros_like(surf)\n",
    "slope_y = np.zeros_like(surf)\n",
    "\n",
    "# Code to calculate slope at each node (find slope between previous and next point)\n",
    "\n",
    "I = len(slope_x)\n",
    "J = len(slope_x[0])\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        z10 = surf[i-1][j]\n",
    "        z12 = surf[i+1][j]\n",
    "        z01 = surf[i][j-1]\n",
    "        z21 = surf[i][j+1]\n",
    "        slope_x[i][j] = (z21 - z01)/(dist*2)\n",
    "        slope_y[i][j] = (z12 - z10)/(dist*2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f5964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SIA for each point\n",
    "\n",
    "A, rho, g, n = 4.6e-25, 910.0, 9.81, 3\n",
    "\n",
    "vel_x = np.zeros_like(surf)\n",
    "vel_y = np.zeros_like(surf)\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        grad2 = (slope_x[i][j]**2 + slope_y[i][j]**2)**((n-1.0)/2.0)\n",
    "        vel_x[i][j] = -((rho*g)**3.0)*(A/2) * slope_x[i][j] * grad2 * (height[i][j]**4)\n",
    "        vel_y[i][j] = -((rho*g)**3.0)*(A/2) * slope_y[i][j] * grad2 * (height[i][j]**4)\n",
    "\n",
    "vel_x = vel_x * (365.25*24*60*60)\n",
    "vel_y = vel_y * (365.25*24*60*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4851f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points outside of polygon\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        boolean = poly.contains(Point(x[i], y[j]))\n",
    "        if boolean == False:\n",
    "            surf[j][i] = np.nan\n",
    "            vel_x[j][i] = np.nan\n",
    "            vel_y[j][i] = np.nan\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b34250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate horizontal strain rate tensor at each node\n",
    "\n",
    "exx = np.zeros_like(surf)\n",
    "eyy = np.zeros_like(surf)\n",
    "exy = np.zeros_like(surf)\n",
    "\n",
    "d = 25\n",
    "for j in range(1, J-1):\n",
    "    for i in range(1, I-1):\n",
    "        di = d\n",
    "        dj = d\n",
    "        while i<=di or i>=I-di:\n",
    "            di = di - 1\n",
    "        while j<=dj or j>=J-dj:\n",
    "            dj = dj - 1\n",
    "        print(i,di,j,dj)\n",
    "        while any([ np.isnan(vel_x[i][j+dj]), np.isnan(vel_x[i][j-dj])]) and dj > 1:\n",
    "            dj = dj - 1\n",
    "        while any([ np.isnan(vel_x[i+di][j]), np.isnan(vel_x[i-di][j])]) and di > 1:\n",
    "            di = di - 1\n",
    "        exx[i][j] = (vel_x[i][j+dj] - vel_x[i][j-dj])/(dj*dist*2)\n",
    "        eyy[i][j] = (vel_y[i+di][j] - vel_y[i-di][j])/(di*dist*2)\n",
    "        exy[i][j] = 0.5*((vel_y[i][j+dj] - vel_y[i][j-dj])/(dj*dist*2)\\\n",
    "                       + (vel_x[i+di][j] - vel_x[i-di][j])/(di*dist*2))\n",
    "        print(\"hi\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eigenvector data structure initialisation\n",
    "v11 = np.zeros_like(surf) # First component of the first vector\n",
    "v12 = np.zeros_like(surf) # Second component of the first vector\n",
    "v21 = np.zeros_like(surf) # First component of the second vector\n",
    "v22 = np.zeros_like(surf) # Second component of the second vector\n",
    "\n",
    "# Eigenvalue data structure initialisation\n",
    "e1 = np.zeros_like(surf)\n",
    "e2 = np.zeros_like(surf)\n",
    "\n",
    "# Calculate strain rate eigenvectors and eigenvalues\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        if ((maths.isnan(exx[i][j])) or (maths.isnan(eyy[i][j])) or (maths.isnan(exy[i][j]))):\n",
    "            e1[i][j], e2[i][j] = np.nan, np.nan\n",
    "            v11[i][j], v12[i][j] = np.nan, np.nan\n",
    "            v21[i][j], v22[i][j] = np.nan, np.nan\n",
    "        else:\n",
    "            arr = np.array([[exx[i][j], exy[i][j]], \n",
    "                            [exy[i][j], eyy[i][j]]])\n",
    "            w,v = eig(arr)\n",
    "            e1[i][j], e2[i][j] = w\n",
    "            v11[i][j], v12[i][j] = v[0]\n",
    "            v21[i][j], v22[i][j] = v[1]\n",
    "\n",
    "#for i in range(1, I-1):\n",
    "#    for j in range(1, J-1):\n",
    "#        if ((maths.isnan(exx[i][j])) or (maths.isnan(eyy[i][j])) or (maths.isnan(exy[i][j]))):\n",
    "#            break\n",
    "#        arr = np.array([[exx[i][j], exy[i][j]], \n",
    "#                        [exy[i][j], eyy[i][j]]])\n",
    "#        w,v = eig(arr)\n",
    "#        e1[i][j], e2[i][j] = w\n",
    "#        v11[i][j], v12[i][j] = v[0]\n",
    "#        v21[i][j], v22[i][j] = v[1]\n",
    "\n",
    "# Weight eigenvectors based on eigenvalues\n",
    "\n",
    "#v11 = np.multiply(v11,e1)\n",
    "#v12 = np.multiply(v12,e1)\n",
    "#v21 = np.multiply(v21,e2)\n",
    "#v22 = np.multiply(v22,e2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an array of the eigenvector correspondsing with the largest eigenvalue\n",
    "\n",
    "v1_max = np.zeros_like(surf)\n",
    "v2_max = np.zeros_like(surf)\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        if e1[i][j] > e2[i][j]:\n",
    "            v1_max[i][j] = v11[i][j]\n",
    "            v2_max[i][j] = v12[i][j]\n",
    "        else:\n",
    "            v1_max[i][j] = v21[i][j]\n",
    "            v2_max[i][j] = v22[i][j]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total strain at each node\n",
    "\n",
    "e_total = np.zeros_like(surf)\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        e_total[i][j] = (exx[i][j] + eyy[i][j])/2\n",
    "\n",
    "# Add a tiny amount to every element\n",
    "\n",
    "#e_total = e_total + 0.0001        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array with velocity magnitudes\n",
    "\n",
    "vel_mag = np.zeros_like(surf)\n",
    "\n",
    "for i in range(1, I-1):\n",
    "    for j in range(1, J-1):\n",
    "        vel_mag[i][j] = np.sqrt(vel_x[i][j]**2 + vel_y[i][j]**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points outside of polygon\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        boolean = poly.contains(Point(x[i], y[j]))\n",
    "        if boolean == False:\n",
    "            surf[j][i] = \"nan\"\n",
    "            vel_mag[j][i] = \"nan\"\n",
    "            vel_x[j][i] = \"nan\"\n",
    "            vel_y[j][i] = \"nan\"\n",
    "            v11[j][i] = \"nan\"\n",
    "            v12[j][i] = \"nan\"\n",
    "            v21[j][i] = \"nan\"\n",
    "            v22[j][i] = \"nan\"\n",
    "            v1_max[j][i] = \"nan\"\n",
    "            v2_max[j][i] = \"nan\"\n",
    "            e_total[j][i] = \"nan\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d80605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide velocities by their magnitude for normalised vectors\n",
    "\n",
    "#vel_x = vel_x / vel_mag\n",
    "#vel_y = vel_y / vel_mag\n",
    "\n",
    "vel_x_norm = np.divide(vel_x, vel_mag, out=np.zeros_like(vel_x), where=vel_mag!=0)\n",
    "vel_y_norm = np.divide(vel_y, vel_mag, out=np.zeros_like(vel_y), where=vel_mag!=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37060d00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "levels1 = np.linspace(0,20,500)\n",
    "CS = ax.contourf(grid_x/1000-770, grid_y/1000-1960, vel_mag, levels=levels1, cmap=plt.cm.GnBu, locator=ticker.LogLocator(), extend='max')\n",
    "\n",
    "skip1 = (slice(None, None, 20), slice(None, None, 20))\n",
    "ax.quiver(grid_x[skip1]/1000-770, grid_y[skip1]/1000-1960, vel_x_norm[skip1], vel_y_norm[skip1], scale=40)\n",
    "ax.plot(data_GL_reduced['X']/1000-770, data_GL_reduced['Y']/1000-1960, color='black')\n",
    "ax.set_xlabel(r'$x$ [km]', size=20)\n",
    "ax.set_ylabel(r'$y$ [km]', size=20)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "cbar = fig.colorbar(CS, ticks=[0, 5, 10, 15, 20])\n",
    "cbar.set_label('Velocity magnitude [km]', size=20)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "fig.savefig('HammarryggenVelocitySmoothing5.jpg', format='jpg', dpi=700, bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b802ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "levels2 = np.linspace(-0.001,0.001,500)\n",
    "#levels2 = np.linspace(0,0.005,500)\n",
    "CS = ax.contourf(grid_x, grid_y, e_total, levels=levels2, cmap=plt.cm.GnBu, extend='max')\n",
    "#CS = ax.contourf(grid_x, grid_y, e_total, levels=levels2, cmap=plt.cm.GnBu, locator=ticker.LogLocator(), extend='max')\n",
    "\n",
    "skip2 = (slice(None, None, 20), slice(None, None, 20))\n",
    "ax.quiver(grid_x[skip2], grid_y[skip2], v1_max[skip2], v2_max[skip2], scale=40, pivot='mid', headlength=0, headwidth=1)\n",
    "ax.plot(data_GL_reduced['X'], data_GL_reduced['Y'], color='black')\n",
    "cbar = fig.colorbar(CS)\n",
    "#ax.tick_params(color='w', labelcolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv of the velocity data\n",
    "\n",
    "# Flatten each array\n",
    "\n",
    "vx = vel_x.flatten()\n",
    "vy = vel_y.flatten()\n",
    "vmag = vel_mag.flatten()\n",
    "xcoord = grid_x.flatten()\n",
    "ycoord = grid_y.flatten()\n",
    "\n",
    "HammarryggenVelData = np.array([xcoord, ycoord, vx, vy, vmag ])\n",
    "HammarryggenVelData = HammarryggenVelData.T\n",
    "np.savetxt(\"HammarryggenVelData.csv\", HammarryggenVelData, delimiter=\",\",  header=\"xcoord,ycoord,vx,vy,vmag\", comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f780421",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vx), len(vy), len(vmag), len(xcoord), len(ycoord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HammarryggenVelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12238fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31121940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120fb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460b13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
